{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BorowskiKacper/llm_multiagent_debate/blob/main/mmlu_Multi_Agent_Review_Board.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3uMELIb3QTPt"
      },
      "outputs": [],
      "source": [
        "# README\n",
        "\n",
        "# Multi agent review board notebook using \"mmlu\" data (Source code has \"geography\", \"gsm\", \"math\", and \"mmlu\" data)\n",
        "# Source code: https://github.com/composable-models/llm_multiagent_debate\n",
        "\n",
        "# CONFIGURE parameters like which models to use and agent configs using fourth code block/cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJgfHKS40a4W"
      },
      "outputs": [],
      "source": [
        "# Install depencies and authenticate with hugging face to access gated models\n",
        "!pip install transformers accelerate -q\n",
        "\n",
        "# Authenticate with Hugging Face\n",
        "from huggingface_hub import login\n",
        "login()  # You'll need a HF token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GH-l3bNs-PgQ"
      },
      "outputs": [],
      "source": [
        "# CONFIGURE rounds, questions, pipelines, and agent configs in this cell\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "# =====ROUNDS & QUESTIONS=====\n",
        "rounds = 2\n",
        "questions = 100\n",
        "\n",
        "# =====GENERATION PIPELINES=====\n",
        "# NOTE: You can create pipelines from multiple models, not just one.\n",
        "# But beware that you have a limited amount of RAM and VRAM to work withRuntime -> View Resources\n",
        "model_id=\"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# =====AGENTS=====\n",
        "# Currently each agent uses the same model/pipeline, but different temperatures\n",
        "agent_configs = [{\"pipe\": pipe, \"temp\": 0.1}, {\"pipe\": pipe, \"temp\": 0.7}, {\"pipe\": pipe, \"temp\": 1.5}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnM-IRkhqvgd"
      },
      "outputs": [],
      "source": [
        "from glob import glob\n",
        "import pandas as pd\n",
        "import json\n",
        "import time\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lH-hlKLL8Ne"
      },
      "outputs": [],
      "source": [
        "# Load MMLU data and store data frames\n",
        "!curl \"https://people.eecs.berkeley.edu/~hendrycks/data.tar\" | tar -xvf -\n",
        "\n",
        "tasks = glob(\"./data/test/*.csv\")\n",
        "dfs = [pd.read_csv(task) for task in tasks]\n",
        "\n",
        "len(dfs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGrgiZy3rzdJ"
      },
      "outputs": [],
      "source": [
        "def construct_message(agents, question, idx):\n",
        "    if len(agents) == 0:\n",
        "        return {\"role\": \"user\", \"content\": \"Can you double check that your answer is correct. Put your final answer in the form (X) at the end of your response.\"}\n",
        "\n",
        "    prefix_string = \"These are the solutions to the problem from other agents: \"\n",
        "\n",
        "    for agent in agents:\n",
        "        agent_response = agent[idx][\"content\"]\n",
        "        response = \"\\n\\n One agent solution: ```{}```\".format(agent_response)\n",
        "\n",
        "        prefix_string = prefix_string + response\n",
        "\n",
        "    prefix_string = prefix_string + \"\"\"\\n\\n Using the reasoning from other agents as additional advice, can you give an updated answer? Examine your solution and that other agents step by step. Put your answer in the form (X) at the end of your response.\"\"\".format(question)\n",
        "    return {\"role\": \"user\", \"content\": prefix_string}\n",
        "\n",
        "\n",
        "def construct_assistant_message(completion):\n",
        "    # content = completion[\"choices\"][0][\"message\"][\"content\"]\n",
        "    content = completion[0][\"generated_text\"][-1][\"content\"]\n",
        "    return {\"role\": \"assistant\", \"content\": content}\n",
        "\n",
        "\n",
        "def generate_answer(agent_config, agent_context):\n",
        "    # try:\n",
        "\n",
        "    #     completion = openai.ChatCompletion.create(\n",
        "    #               model=\"gpt-3.5-turbo-0301\",\n",
        "    #               messages=answer_context,\n",
        "    #               n=1)\n",
        "    # except:\n",
        "    #     print(\"retrying due to an error......\")\n",
        "    #     time.sleep(20)\n",
        "    #     return generate_answer(answer_context)\n",
        "    completion = agent_config[\"pipe\"](\n",
        "      agent_context,\n",
        "      max_new_tokens=1024,\n",
        "      do_sample=True,\n",
        "      temperature=agent_config[\"temp\"],\n",
        "      top_p=0.9,\n",
        "    )\n",
        "\n",
        "\n",
        "    return completion\n",
        "\n",
        "\n",
        "def parse_question_answer(df, ix):\n",
        "    question = df.iloc[ix, 0]\n",
        "    a = df.iloc[ix, 1]\n",
        "    b = df.iloc[ix, 2]\n",
        "    c = df.iloc[ix, 3]\n",
        "    d = df.iloc[ix, 4]\n",
        "\n",
        "    question = \"Can you answer the following question as accurately as possible? {}: A) {}, B) {}, C) {}, D) {} Explain your answer, putting the answer in the form (X) at the end of your response.\".format(question, a, b, c, d)\n",
        "\n",
        "    answer = df.iloc[ix, 5]\n",
        "\n",
        "    return question, answer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "caXFtOcPwnQM"
      },
      "outputs": [],
      "source": [
        "random.seed(0)\n",
        "response_dict = {}\n",
        "\n",
        "for i in range(questions):\n",
        "    df = random.choice(dfs)\n",
        "    ix = len(df)\n",
        "    idx = random.randint(0, ix-1)\n",
        "\n",
        "    question, answer = parse_question_answer(df, idx)\n",
        "\n",
        "    # agent_contexts = [[{\"role\": \"user\", \"content\": question}] for agent in range(agents)]\n",
        "    agent_contexts = [[{\"role\": \"user\", \"content\": question}] for agent in range(len(agent_configs))]\n",
        "    print(f\"Question {i+1}/{questions}: {question}\")\n",
        "    print(f\"Answer: {answer}\")\n",
        "\n",
        "    for round in range(rounds):\n",
        "        for i, agent_context in enumerate(agent_contexts):\n",
        "            print(f\"Round: {round + 1}/{rounds} | Agent: {i+1}/{len(agent_configs)}\")\n",
        "            if round != 0:\n",
        "                agent_contexts_other = agent_contexts[:i] + agent_contexts[i+1:]\n",
        "                message = construct_message(agent_contexts_other, question, 2 * round - 1)\n",
        "                agent_context.append(message)\n",
        "\n",
        "            completion = generate_answer(agent_configs[i], agent_context)\n",
        "\n",
        "            assistant_message = construct_assistant_message(completion)\n",
        "            agent_context.append(assistant_message)\n",
        "            print(assistant_message)\n",
        "\n",
        "    response_dict[question] = (agent_contexts, answer)\n",
        "\n",
        "json.dump(response_dict, open(\"mmlu_{}_{}.json\".format(len(agent_configs), rounds), \"w\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eval"
      ],
      "metadata": {
        "id": "lM1aACxiDpH4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "import json\n",
        "from collections import Counter\n",
        "import re"
      ],
      "metadata": {
        "id": "7o3yxA5yDrDB"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_answer(input_str):\n",
        "    pattern = r'\\((\\w)\\)'\n",
        "    matches = re.findall(pattern, input_str)\n",
        "\n",
        "    solution = None\n",
        "\n",
        "    for match_str in matches[::-1]:\n",
        "        solution = match_str.upper()\n",
        "        if solution:\n",
        "            break\n",
        "\n",
        "    return solution\n",
        "\n",
        "\n",
        "def parsed_answers(pred_solutions):\n",
        "    if type(pred_solutions) == list:\n",
        "        pred_answers = []\n",
        "\n",
        "        for pred_solution in pred_solutions:\n",
        "            pred_answer = parse_answer(pred_solution)\n",
        "            pred_answers.append(pred_answer)\n",
        "\n",
        "    else:\n",
        "        pred_answers = [parse_answer(pred_solutions)]\n",
        "\n",
        "    return pred_answers\n",
        "\n",
        "\n",
        "def most_frequent(letters):\n",
        "  if not letters:\n",
        "    raise ValueError(\"Expected non-empty list.\")\n",
        "\n",
        "  return Counter(letters).most_common(1)[0][0]"
      ],
      "metadata": {
        "id": "Taw4UauvLgFz"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentStats:\n",
        "  def __init__(self):\n",
        "    self.counts = {}\n",
        "    self.correct_count = 0\n",
        "\n",
        "  def add_stat(self, predicted_answer, gt):\n",
        "    self.counts[predicted_answer] = self.counts.get(predicted_answer, 0) + 1\n",
        "    self.correct_count += predicted_answer == gt\n",
        "\n",
        "  def __str__(self):\n",
        "    return f\"Counts: {self.counts}\\nCorrect: {self.correct_count}\\n=========================\"\n"
      ],
      "metadata": {
        "id": "hBB5J43N_Kpq"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Inference results\n",
        "response_dict = json.load(open(\"mmlu_3_2.json\", \"r\"))\n",
        "questions = list(response_dict.keys())\n",
        "\n",
        "# Store individual answers, consensus answers, and ground truth for each question\n",
        "results = []\n",
        "for question in questions:\n",
        "    responses, gt = response_dict[question]\n",
        "\n",
        "    pred_solutions = []\n",
        "    for response in responses:\n",
        "        pred_solution = response[-1]['content']\n",
        "        pred_solutions.append(pred_solution)\n",
        "\n",
        "\n",
        "    pred_answers = parsed_answers(pred_solutions) # Individual answers\n",
        "    consensus = most_frequent(pred_answers) # Consensus\n",
        "    res = [pred_answers, consensus, gt]\n",
        "    results.append(res)\n",
        "\n",
        "# Parse stats for each agent answer and consensus answers\n",
        "agents = [AgentStats() for _ in range(len(results[0]))]\n",
        "group = AgentStats()\n",
        "for answers, consensus, gt in results:\n",
        "  # Individual eval\n",
        "  for i, answer in enumerate(answers):\n",
        "    agents[i].add_stat(answer, gt)\n",
        "\n",
        "  # Group eval\n",
        "  group.add_stat(consensus, gt)\n",
        "\n",
        "print(\"---------------Individual Agents---------------\")\n",
        "for agent in agents:\n",
        "  print(agent)\n",
        "\n",
        "print(\"---------------Group results---------------\")\n",
        "print(group)\n",
        "\n",
        "print(f\"Total Questions: {len(questions)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71cW8C9TIBAn",
        "outputId": "9d6ee3f3-963b-47af-ae6f-0b3332ce82da"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------Individual Agents---------------\n",
            "Counts: {'A': 8, 'C': 5, 'B': 4, 'X': 51, 'D': 9, None: 15, 'E': 2, 'I': 1, 'Y': 1, '1': 1, 'T': 1, 'G': 1}\n",
            "Correct: 9\n",
            "=========================\n",
            "Counts: {'X': 42, None: 17, 'B': 7, 'Y': 1, 'D': 12, 'C': 12, 'A': 5, '1': 1, 'W': 1, 'E': 1}\n",
            "Correct: 15\n",
            "=========================\n",
            "Counts: {'A': 12, 'D': 11, None: 21, 'X': 29, '4': 1, 'B': 12, 'C': 5, 'I': 1, '2': 2, 'W': 2, '8': 1, 'F': 1, 'Î’': 1}\n",
            "Correct: 14\n",
            "=========================\n",
            "---------------Group results---------------\n",
            "Counts: {'A': 9, 'C': 6, 'B': 7, 'X': 50, 'D': 7, None: 15, 'E': 1, 'I': 1, 'Y': 1, '1': 1, 'W': 1}\n",
            "Correct: 13\n",
            "=========================\n",
            "Total Questions: 99\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}