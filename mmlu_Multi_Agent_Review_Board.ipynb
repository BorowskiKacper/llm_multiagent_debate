{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3uMELIb3QTPt"
      },
      "outputs": [],
      "source": [
        "# README\n",
        "\n",
        "# Multi agent review board notebook using \"mmlu\" data (Source code has \"geography\", \"gsm\", \"math\", and \"mmlu\" data)\n",
        "# Source code: https://github.com/composable-models/llm_multiagent_debate\n",
        "\n",
        "# CONFIGURE parameters like which models to use and agent configs using fourth code block/cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJgfHKS40a4W"
      },
      "outputs": [],
      "source": [
        "# Install depencies and authenticate with hugging face to access gated models\n",
        "!pip install transformers accelerate -q\n",
        "\n",
        "# Authenticate with Hugging Face\n",
        "from huggingface_hub import login\n",
        "login()  # You'll need a HF token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GH-l3bNs-PgQ"
      },
      "outputs": [],
      "source": [
        "# CONFIGURE rounds, questions, pipelines, and agent configs in this cell\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "# =====ROUNDS & QUESTIONS=====\n",
        "rounds = 2\n",
        "questions = 100\n",
        "\n",
        "# =====GENERATION PIPELINES=====\n",
        "# NOTE: You can create pipelines from multiple models, not just one.\n",
        "# But beware that you have a limited amount of RAM and VRAM to work withRuntime -> View Resources\n",
        "model_id=\"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# =====AGENTS=====\n",
        "# Currently each agent uses the same model/pipeline, but different temperatures\n",
        "agent_configs = [{\"pipe\": pipe, \"temp\": 0.1}, {\"pipe\": pipe, \"temp\": 0.7}, {\"pipe\": pipe, \"temp\": 1.5}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnM-IRkhqvgd"
      },
      "outputs": [],
      "source": [
        "from glob import glob\n",
        "import pandas as pd\n",
        "import json\n",
        "import time\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lH-hlKLL8Ne"
      },
      "outputs": [],
      "source": [
        "# Load MMLU data and store data frames\n",
        "!curl \"https://people.eecs.berkeley.edu/~hendrycks/data.tar\" | tar -xvf -\n",
        "\n",
        "tasks = glob(\"./data/test/*.csv\")\n",
        "dfs = [pd.read_csv(task) for task in tasks]\n",
        "\n",
        "len(dfs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGrgiZy3rzdJ"
      },
      "outputs": [],
      "source": [
        "def construct_message(agents, question, idx):\n",
        "    if len(agents) == 0:\n",
        "        return {\"role\": \"user\", \"content\": \"Can you double check that your answer is correct. Put your final answer in the form (X) at the end of your response.\"}\n",
        "\n",
        "    prefix_string = \"These are the solutions to the problem from other agents: \"\n",
        "\n",
        "    for agent in agents:\n",
        "        agent_response = agent[idx][\"content\"]\n",
        "        response = \"\\n\\n One agent solution: ```{}```\".format(agent_response)\n",
        "\n",
        "        prefix_string = prefix_string + response\n",
        "\n",
        "    prefix_string = prefix_string + \"\"\"\\n\\n Using the reasoning from other agents as additional advice, can you give an updated answer? Examine your solution and that other agents step by step. Put your answer in the form (X) at the end of your response.\"\"\".format(question)\n",
        "    return {\"role\": \"user\", \"content\": prefix_string}\n",
        "\n",
        "\n",
        "def construct_assistant_message(completion):\n",
        "    # content = completion[\"choices\"][0][\"message\"][\"content\"]\n",
        "    content = completion[0][\"generated_text\"][-1][\"content\"]\n",
        "    return {\"role\": \"assistant\", \"content\": content}\n",
        "\n",
        "\n",
        "def generate_answer(agent_config, agent_context):\n",
        "    # try:\n",
        "\n",
        "    #     completion = openai.ChatCompletion.create(\n",
        "    #               model=\"gpt-3.5-turbo-0301\",\n",
        "    #               messages=answer_context,\n",
        "    #               n=1)\n",
        "    # except:\n",
        "    #     print(\"retrying due to an error......\")\n",
        "    #     time.sleep(20)\n",
        "    #     return generate_answer(answer_context)\n",
        "    completion = agent_config[\"pipe\"](\n",
        "      agent_context,\n",
        "      max_new_tokens=1024,\n",
        "      do_sample=True,\n",
        "      temperature=agent_config[\"temp\"],\n",
        "      top_p=0.9,\n",
        "    )\n",
        "\n",
        "\n",
        "    return completion\n",
        "\n",
        "\n",
        "def parse_question_answer(df, ix):\n",
        "    question = df.iloc[ix, 0]\n",
        "    a = df.iloc[ix, 1]\n",
        "    b = df.iloc[ix, 2]\n",
        "    c = df.iloc[ix, 3]\n",
        "    d = df.iloc[ix, 4]\n",
        "\n",
        "    question = \"Can you answer the following question as accurately as possible? {}: A) {}, B) {}, C) {}, D) {} Explain your answer, putting the answer in the form (X) at the end of your response.\".format(question, a, b, c, d)\n",
        "\n",
        "    answer = df.iloc[ix, 5]\n",
        "\n",
        "    return question, answer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "caXFtOcPwnQM"
      },
      "outputs": [],
      "source": [
        "random.seed(0)\n",
        "response_dict = {}\n",
        "\n",
        "for i in range(questions):\n",
        "    df = random.choice(dfs)\n",
        "    ix = len(df)\n",
        "    idx = random.randint(0, ix-1)\n",
        "\n",
        "    question, answer = parse_question_answer(df, idx)\n",
        "\n",
        "    # agent_contexts = [[{\"role\": \"user\", \"content\": question}] for agent in range(agents)]\n",
        "    agent_contexts = [[{\"role\": \"user\", \"content\": question}] for agent in range(len(agent_configs))]\n",
        "    print(f\"Question {i+1}/{questions}: {question}\")\n",
        "    print(f\"Answer: {answer}\")\n",
        "\n",
        "    for round in range(rounds):\n",
        "        for i, agent_context in enumerate(agent_contexts):\n",
        "            print(f\"Round: {round + 1}/{rounds} | Agent: {i+1}/{len(agent_configs)}\")\n",
        "            if round != 0:\n",
        "                agent_contexts_other = agent_contexts[:i] + agent_contexts[i+1:]\n",
        "                message = construct_message(agent_contexts_other, question, 2 * round - 1)\n",
        "                agent_context.append(message)\n",
        "\n",
        "            completion = generate_answer(agent_configs[i], agent_context)\n",
        "\n",
        "            assistant_message = construct_assistant_message(completion)\n",
        "            agent_context.append(assistant_message)\n",
        "            print(assistant_message)\n",
        "\n",
        "    response_dict[question] = (agent_contexts, answer)\n",
        "\n",
        "json.dump(response_dict, open(\"mmlu_{}_{}.json\".format(len(agent_configs), rounds), \"w\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eval"
      ],
      "metadata": {
        "id": "lM1aACxiDpH4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "import json\n",
        "from collections import Counter\n",
        "import re"
      ],
      "metadata": {
        "id": "7o3yxA5yDrDB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import json result\n"
      ],
      "metadata": {
        "id": "eStjUx-cLLIj"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# def solve_math_problems(input_str):\n",
        "#     pattern = r\"\\d+\\.?\\d*\"\n",
        "\n",
        "#     matches = re.findall(pattern, input_str)\n",
        "#     if matches:\n",
        "#         return matches[-1]\n",
        "\n",
        "#     return None\n",
        "\n",
        "def parse_answer(input_str):\n",
        "    pattern = r'\\((\\w)\\)'\n",
        "    matches = re.findall(pattern, input_str)\n",
        "\n",
        "    solution = None\n",
        "    # print(\"predicted solution\")\n",
        "    # print(input_str)\n",
        "    # print(\"matches\")\n",
        "    # print(matches)\n",
        "\n",
        "    for match_str in matches[::-1]:\n",
        "        solution = match_str.upper()\n",
        "        if solution:\n",
        "            break\n",
        "\n",
        "    return solution\n",
        "\n",
        "\n",
        "def parsed_answers(pred_solutions):\n",
        "    if type(pred_solutions) == list:\n",
        "        pred_answers = []\n",
        "\n",
        "        for pred_solution in pred_solutions:\n",
        "            pred_answer = parse_answer(pred_solution)\n",
        "            pred_answers.append(pred_answer)\n",
        "\n",
        "        # if pred_answer is None:\n",
        "        #     return 0\n",
        "        # pred_answer = most_frequent(pred_answers)\n",
        "    else:\n",
        "        pred_answers = [parse_answer(pred_solutions)]\n",
        "        # if pred_answer is None:\n",
        "        #     pred_answer = solve_math_problems(pred_solutions)\n",
        "\n",
        "    return pred_answers\n",
        "\n",
        "\n",
        "def most_frequent(letters):\n",
        "  # Given a list of letters (ex: [\"A\", \"A\", \"C\"]), return the first most frequent letter\n",
        "  if not letters:\n",
        "    raise ValueError(\"Expected non-empty list.\")\n",
        "\n",
        "  return Counter(letters).most_common(1)[0][0]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Taw4UauvLgFz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_dict = json.load(open(\"mmlu_3_2.json\", \"r\"))\n",
        "questions = list(response_dict.keys())\n",
        "\n",
        "accuracies = []\n",
        "\n",
        "i = 19\n",
        "\n",
        "results = []\n",
        "for question in questions:\n",
        "    responses, gt = response_dict[question]\n",
        "\n",
        "    pred_solutions = []\n",
        "    for response in responses:\n",
        "        pred_solution = response[-1]['content']\n",
        "        pred_solutions.append(pred_solution)\n",
        "        # print(pred_solution)\n",
        "        # print(\"--------------------------\")\n",
        "\n",
        "\n",
        "    pred_answers = parsed_answers(pred_solutions)\n",
        "    consensus = most_frequent(pred_answers)\n",
        "    # print(\"Predicted: \", pred_answers)\n",
        "    # print(\"Consensus: \", consensus)\n",
        "    # print(\"Actual: \", gt)\n",
        "    # print(\"=========================\")\n",
        "\n",
        "    # res = {\"Predicted\": pred_answers,\n",
        "    #        \"Consensus\": consensus,\n",
        "    #        \"Actual\": gt}\n",
        "    res = [pred_answers, consensus, gt]\n",
        "    # print(f\"Predicted: {pred_answers} | Consensus: {consensus} | Actual: {gt}\")\n",
        "    print(res)\n",
        "    results.append(res)\n",
        "\n",
        "    if i == 0:\n",
        "      break\n",
        "    else:\n",
        "      i -= 1\n",
        "\n",
        "print(results)\n",
        "# agent1 = {}\n",
        "# agent2 = {}\n",
        "# agent3 = {}\n",
        "\n",
        "def eval_agent(predicted_answer, gt, agent):\n",
        "  agent[\"counts\"][predicted_answer] = agent[\"counts\"].get(predicted_answer, 0) + 1\n",
        "  agent[\"correct_count\"] += predicted_answer == gt\n",
        "\n",
        "agents = [{\"counts\": {}, \"correct_count\": 0}, {\"counts\": {}, \"correct_count\": 0}, {\"counts\": {}, \"correct_count\": 0}] # [{\"counts\": {char/letter/answer: count}, \"correct_count\": int}]\n",
        "group = {\"counts\": {}, \"correct_count\": 0}  # {\"counts\": {char/letter/answer: count}, \"correct_count\": int}\n",
        "for answers, consensus, gt in results:\n",
        "  # Individual eval\n",
        "  for i, answer in enumerate(answers):\n",
        "    eval_agent(answer, gt, agents[i])\n",
        "\n",
        "  # Group eval\n",
        "  eval_agent(answer, consensus, group)\n",
        "\n",
        "print(agents)\n",
        "print(group)\n",
        "\n",
        "\n",
        "    # # pred_solutions = pred_solutions[:1]\n",
        "\n",
        "    # # accurate = compute_accuracy(gt, pred_solutions)\n",
        "    # predicted_answer = answer_consensus(pred_solutions)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # if accurate is not None:\n",
        "    #     accuracies.append(float(accurate))\n",
        "    # else:\n",
        "    #     import pdb\n",
        "    #     pdb.set_trace()\n",
        "    #     print(gt)\n",
        "\n",
        "    # print(\"accuracies:\", np.mean(accuracies), np.std(accuracies) / (len(accuracies) ** 0.5))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71cW8C9TIBAn",
        "outputId": "f257feb2-456a-43c4-c12f-1c01b4e37bc0"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['A', 'X', 'A'], 'A', 'A']\n",
            "[['C', None, 'D'], 'C', 'D']\n",
            "[['B', 'B', None], 'B', 'B']\n",
            "[['C', 'X', 'D'], 'C', 'B']\n",
            "[['X', 'X', 'X'], 'X', 'C']\n",
            "[['D', 'Y', '4'], 'D', 'D']\n",
            "[['X', 'X', None], 'X', 'D']\n",
            "[['X', 'B', 'X'], 'X', 'C']\n",
            "[['X', 'D', 'B'], 'X', 'C']\n",
            "[['D', 'D', 'D'], 'D', 'D']\n",
            "[['A', 'C', 'B'], 'A', 'A']\n",
            "[[None, None, 'C'], None, 'B']\n",
            "[['E', 'X', 'C'], 'E', 'C']\n",
            "[['X', 'X', 'B'], 'X', 'D']\n",
            "[['I', 'X', 'I'], 'I', 'C']\n",
            "[['X', 'D', None], 'X', 'A']\n",
            "[['D', None, None], None, 'C']\n",
            "[['X', 'X', 'X'], 'X', 'C']\n",
            "[['X', 'X', 'D'], 'X', 'C']\n",
            "[['Y', 'D', '2'], 'Y', 'D']\n",
            "[[['A', 'X', 'A'], 'A', 'A'], [['C', None, 'D'], 'C', 'D'], [['B', 'B', None], 'B', 'B'], [['C', 'X', 'D'], 'C', 'B'], [['X', 'X', 'X'], 'X', 'C'], [['D', 'Y', '4'], 'D', 'D'], [['X', 'X', None], 'X', 'D'], [['X', 'B', 'X'], 'X', 'C'], [['X', 'D', 'B'], 'X', 'C'], [['D', 'D', 'D'], 'D', 'D'], [['A', 'C', 'B'], 'A', 'A'], [[None, None, 'C'], None, 'B'], [['E', 'X', 'C'], 'E', 'C'], [['X', 'X', 'B'], 'X', 'D'], [['I', 'X', 'I'], 'I', 'C'], [['X', 'D', None], 'X', 'A'], [['D', None, None], None, 'C'], [['X', 'X', 'X'], 'X', 'C'], [['X', 'X', 'D'], 'X', 'C'], [['Y', 'D', '2'], 'Y', 'D']]\n",
            "[{'counts': {'A': 2, 'C': 2, 'B': 1, 'X': 8, 'D': 3, None: 1, 'E': 1, 'I': 1, 'Y': 1}, 'correct_count': 5}, {'counts': {'X': 9, None: 3, 'B': 2, 'Y': 1, 'D': 4, 'C': 1}, 'correct_count': 3}, {'counts': {'A': 1, 'D': 4, None: 4, 'X': 3, '4': 1, 'B': 3, 'C': 2, 'I': 1, '2': 1}, 'correct_count': 4}]\n",
            "{'counts': {'A': 1, 'D': 4, None: 4, 'X': 3, '4': 1, 'B': 3, 'C': 2, 'I': 1, '2': 1}, 'correct_count': 7}\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}