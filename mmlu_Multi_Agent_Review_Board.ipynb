{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BorowskiKacper/llm_multiagent_debate/blob/main/Multi_Agent_Review_Board.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3uMELIb3QTPt"
      },
      "outputs": [],
      "source": [
        "# README\n",
        "\n",
        "# Multi agent review board notebook using \"mmlu\" data (Source code has \"geography\", \"gsm\", \"math\", and \"mmlu\" data)\n",
        "# Source code: https://github.com/composable-models/llm_multiagent_debate\n",
        "\n",
        "# CONFIGURE parameters like which models to use and agent configs using fourth code block/cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJgfHKS40a4W"
      },
      "outputs": [],
      "source": [
        "# Install depencies and authenticate with hugging face to access gated models\n",
        "!pip install transformers accelerate -q\n",
        "\n",
        "# Authenticate with Hugging Face\n",
        "from huggingface_hub import login\n",
        "login()  # You'll need a HF token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GH-l3bNs-PgQ"
      },
      "outputs": [],
      "source": [
        "# CONFIGURE rounds, questions, pipelines, and agent configs in this cell\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "# =====ROUNDS & QUESTIONS=====\n",
        "rounds = 2\n",
        "questions = 100\n",
        "\n",
        "# =====GENERATION PIPELINES=====\n",
        "# NOTE: You can create pipelines from multiple models, not just one.\n",
        "# But beware that you have a limited amount of RAM and VRAM to work withRuntime -> View Resources\n",
        "model_id=\"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# =====AGENTS=====\n",
        "# Currently each agent uses the same model/pipeline, but different temperatures\n",
        "agent_configs = [{\"pipe\": pipe, \"temp\": 0.1}, {\"pipe\": pipe, \"temp\": 0.7}, {\"pipe\": pipe, \"temp\": 1.5}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnM-IRkhqvgd"
      },
      "outputs": [],
      "source": [
        "from glob import glob\n",
        "import pandas as pd\n",
        "import json\n",
        "import time\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lH-hlKLL8Ne"
      },
      "outputs": [],
      "source": [
        "# Load MMLU data and store data frames\n",
        "!curl \"https://people.eecs.berkeley.edu/~hendrycks/data.tar\" | tar -xvf -\n",
        "\n",
        "tasks = glob(\"./data/test/*.csv\")\n",
        "dfs = [pd.read_csv(task) for task in tasks]\n",
        "\n",
        "len(dfs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGrgiZy3rzdJ"
      },
      "outputs": [],
      "source": [
        "def construct_message(agents, question, idx):\n",
        "    if len(agents) == 0:\n",
        "        return {\"role\": \"user\", \"content\": \"Can you double check that your answer is correct. Put your final answer in the form (X) at the end of your response.\"}\n",
        "\n",
        "    prefix_string = \"These are the solutions to the problem from other agents: \"\n",
        "\n",
        "    for agent in agents:\n",
        "        agent_response = agent[idx][\"content\"]\n",
        "        response = \"\\n\\n One agent solution: ```{}```\".format(agent_response)\n",
        "\n",
        "        prefix_string = prefix_string + response\n",
        "\n",
        "    prefix_string = prefix_string + \"\"\"\\n\\n Using the reasoning from other agents as additional advice, can you give an updated answer? Examine your solution and that other agents step by step. Put your answer in the form (X) at the end of your response.\"\"\".format(question)\n",
        "    return {\"role\": \"user\", \"content\": prefix_string}\n",
        "\n",
        "\n",
        "def construct_assistant_message(completion):\n",
        "    # content = completion[\"choices\"][0][\"message\"][\"content\"]\n",
        "    content = completion[0][\"generated_text\"][-1][\"content\"]\n",
        "    return {\"role\": \"assistant\", \"content\": content}\n",
        "\n",
        "\n",
        "def generate_answer(agent_config, agent_context):\n",
        "    # try:\n",
        "\n",
        "    #     completion = openai.ChatCompletion.create(\n",
        "    #               model=\"gpt-3.5-turbo-0301\",\n",
        "    #               messages=answer_context,\n",
        "    #               n=1)\n",
        "    # except:\n",
        "    #     print(\"retrying due to an error......\")\n",
        "    #     time.sleep(20)\n",
        "    #     return generate_answer(answer_context)\n",
        "    completion = agent_config[\"pipe\"](\n",
        "      agent_context,\n",
        "      max_new_tokens=1024,\n",
        "      do_sample=True,\n",
        "      temperature=agent_config[\"temp\"],\n",
        "      top_p=0.9,\n",
        "    )\n",
        "\n",
        "\n",
        "    return completion\n",
        "\n",
        "\n",
        "def parse_question_answer(df, ix):\n",
        "    question = df.iloc[ix, 0]\n",
        "    a = df.iloc[ix, 1]\n",
        "    b = df.iloc[ix, 2]\n",
        "    c = df.iloc[ix, 3]\n",
        "    d = df.iloc[ix, 4]\n",
        "\n",
        "    question = \"Can you answer the following question as accurately as possible? {}: A) {}, B) {}, C) {}, D) {} Explain your answer, putting the answer in the form (X) at the end of your response.\".format(question, a, b, c, d)\n",
        "\n",
        "    answer = df.iloc[ix, 5]\n",
        "\n",
        "    return question, answer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "caXFtOcPwnQM"
      },
      "outputs": [],
      "source": [
        "random.seed(0)\n",
        "response_dict = {}\n",
        "\n",
        "for i in range(questions):\n",
        "    df = random.choice(dfs)\n",
        "    ix = len(df)\n",
        "    idx = random.randint(0, ix-1)\n",
        "\n",
        "    question, answer = parse_question_answer(df, idx)\n",
        "\n",
        "    # agent_contexts = [[{\"role\": \"user\", \"content\": question}] for agent in range(agents)]\n",
        "    agent_contexts = [[{\"role\": \"user\", \"content\": question}] for agent in range(len(agent_configs))]\n",
        "    print(f\"Question {i+1}/{questions}: {question}\")\n",
        "    print(f\"Answer: {answer}\")\n",
        "\n",
        "    for round in range(rounds):\n",
        "        for i, agent_context in enumerate(agent_contexts):\n",
        "            print(f\"Round: {round + 1}/{rounds} | Agent: {i+1}/{len(agent_configs)}\")\n",
        "            if round != 0:\n",
        "                agent_contexts_other = agent_contexts[:i] + agent_contexts[i+1:]\n",
        "                message = construct_message(agent_contexts_other, question, 2 * round - 1)\n",
        "                agent_context.append(message)\n",
        "\n",
        "            completion = generate_answer(agent_configs[i], agent_context)\n",
        "\n",
        "            assistant_message = construct_assistant_message(completion)\n",
        "            agent_context.append(assistant_message)\n",
        "            print(assistant_message)\n",
        "\n",
        "    response_dict[question] = (agent_contexts, answer)\n",
        "\n",
        "json.dump(response_dict, open(\"mmlu_{}_{}.json\".format(len(agent_configs), rounds), \"w\"))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNyDruGVn4sOQohRFoZ6CW2",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
